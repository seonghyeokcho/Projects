{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = extract1.copy()\n",
    "features = trainset.drop('attack', axis=1)\n",
    "labels = trainset['attack']\n",
    "\n",
    "predset = extract2.copy()\n",
    "preds = predset.drop('attack', axis=1)\n",
    "pred_label = predset['attack']\n",
    "\n",
    "np.shape(features), np.shape(labels), np.shape(preds), np.shape(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedShuffleSplit, RandomizedSearchCV\n",
    "\n",
    "def bestGBDTNextModel(model, isKfold, nfold, searchCV, Xtrain, ytrain, Xtest, ytest, nIter, scoring, errScore, verbose, nJobs):\n",
    "    grd_prams = {}\n",
    "    classifier = LGBMClassifier(random_state=0, objective='binary:logistic')\n",
    "    cv = KFold(n_splits=nfold, shuffle=True, random_state=0)\n",
    "    if model == 'LGBM':  \n",
    "        grd_prams.update({'max_depth': [50, 100],\n",
    "              'learning_rate' : [0.01, 0.05],\n",
    "              'num_leaves': [150, 200],\n",
    "              'n_estimators': [300, 400],\n",
    "              'num_boost_round':[4000, 5000],\n",
    "              'subsample': [0.5, 1],\n",
    "              'reg_alpha': [0.01, 0.1],\n",
    "              'reg_lambda': [0.01, 0.1],\n",
    "              'min_data_in_leaf': [20, 30],\n",
    "              'lambda_l1': [0.01, 0.1],\n",
    "              'lambda_l2': [0.01, 0.1]\n",
    "            })\n",
    "        classifier = LGBMClassifier(random_state=0, boosting_type='gbdt', objective='binary', metric='auc')\n",
    "    if isKfold == False:\n",
    "        cv = StratifiedShuffleSplit(n_splits=nfold, test_size=0.2, random_state=0)\n",
    "    grid_ = RandomizedSearchCV(classifier, param_distributions=grd_prams,\n",
    "                               n_iter=nIter, scoring=scoring, error_score=errScore, verbose=verbose, n_jobs=nJobs, cv=cv)\n",
    "    grid_.fit(Xtrain, ytrain)\n",
    "    score_ = grid_.score(Xtest, ytest)\n",
    "    print(\"{} grid_.best_score {}\".format(model, np.round(grid_.best_score_,3)))\n",
    "    print(\"{} grid_.best_score {}\".format(model, np.round(score_,3)))\n",
    "    print(\"{} best_estimator {}\".format(model, grid_.best_estimator_))\n",
    "    return grid_.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = bestGBDTNextModel('LGBM', False, 5, 'RANDOM', features, labels, preds, pred_label, 15, 'roc_auc', 0, 3, -1)\n",
    "lgbm = LGBMClassifier(**best_param)\n",
    "score_lgbm = lgbm.fit(features, labels).score(preds, pred_label)\n",
    "print(\"score_lgbm ::: {}\".format(score_lgbm))\n",
    "print(\"-----------------------------------\")\n",
    "y_lgbm = lgbm.predict(preds)\n",
    "print(classification_report(pred_label, y_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "               importance_type='split', lambda_l1=0.01, lambda_l2=0.1,\n",
    "               learning_rate=0.05, max_depth=50, metric='auc',\n",
    "               min_child_samples=20, min_child_weight=0.001,\n",
    "               min_data_in_leaf=30, min_split_gain=0.0, n_estimators=300,\n",
    "               n_jobs=-1, num_boost_round=5000, num_leaves=150,\n",
    "               objective='binary', random_state=0, reg_alpha=0.1,\n",
    "               reg_lambda=0.01, silent=True, subsample=0.5,\n",
    "               subsample_for_bin=200000, subsample_freq=0)\n",
    "score = lgbm.fit(X_resampled, y_resampled).score(preds, pred_label)\n",
    "print(\"score_lgbm ::: {}\".format(score))\n",
    "print(\"-----------------------------------\")\n",
    "y_lgbm = lgbm.predict(preds)\n",
    "print(classification_report(pred_label, y_lgbm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = testset1.copy()\n",
    "features = trainset.drop(['attack','attack_P1','attack_P2','attack_P3'], axis=1)\n",
    "labels = trainset['attack']\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "predset = testset2.copy()\n",
    "preds = predset.drop(['attack','attack_P1','attack_P2','attack_P3'], axis=1)\n",
    "pred_label = predset['attack']\n",
    "\n",
    "preds = np.array(preds)\n",
    "pred_label = np.array(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(features), np.shape(labels), np.shape(preds), np.shape(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError, MeanSquaredError\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Bidirectional, RNN, BatchNormalization, TimeDistributed, LeakyReLU\n",
    "from tensorflow.keras.experimental import PeepholeLSTMCell\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(3600, 60, input_length=59))\n",
    "    model.add(layers.Dropout(.3))\n",
    "    model.add(layers.Conv1D(60, 3, padding='valid', activation='relu'))\n",
    "    \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(60, activation='relu'))\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 초기화\n",
    "!rm -rf ./logs/RNN_n_1_LSTM/\n",
    "\n",
    "# 척도\n",
    "#metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "metrics = tf.keras.metrics.Accuracy()\n",
    "\n",
    "# 손실함수\n",
    "#loss = tf.keras.losses.CosineSimilarity()\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# 최적화기\n",
    "#optimizer = tf.keras.optimizers.Adadelta(lr=0.1)  # 종종걸음 너무 작아져서 정지하는걸 막아보자.\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=0.1)  # 전부 다봐야 하는데 한걸음은 너무 오래 걸리니까 조금만 보고 빨리 판단한다. 같은시간에 더 많이 간다.\n",
    "#optimizer = tf.keras.optimizers.RMSprop(lr=0.001)  # 보폭을 줄이는건 좋은데 이전 맥락 상황봐가며 하자.\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)  # RMSProp + Momentum 방향도 스텝사이즈도 적절하게!(Momentum:스텝 계산해서 움직인 후, 아까 내려오던 관성방향 또 가자)\n",
    "#optimizer = tf.keras.optimizers.Adagrad(lr=0.1)  # 안가본곳은 성큼 빠르게 훑고, 많이 가본곳은 잘아니까 갈수록 보폭을 줄여 세밀히 탐색\n",
    "#optimizer = tf.keras.optimizers.Adamax(lr=0.1)\n",
    "#optimizer = tf.keras.optimizers.Nadam(lr=0.01)  # Adam에 Momentum 대신에 NAG를 붙이자(NAG:일단 관성방향 먼저 움직이고, 움직인 자리에 스텝을 계산하니 더 빠르더라)\n",
    "#optimizer = tf.keras.optimizers.Ftrl(lr=0.01)\n",
    "\n",
    "## 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 3600\n",
    "\n",
    "# 훈련\n",
    "data_dir = './model/CNN/'\n",
    "logs_dir = './logs/CNN'\n",
    "best_model_file = os.path.join(data_dir, 'best_model.h5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file, monitor='accuracy', save_weights_only=True, save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, mode='min', restore_best_weights=True)\n",
    "CALLBACKS = [checkpoint, tensorboard]\n",
    "history = model.fit(features, labels,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=CALLBACKS,\n",
    "                    validation_data=[preds, pred_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.evaluate(preds,pred_label, batch_size=BATCH_SIZE)\n",
    "print(\"\\nTest score:\",history[0])\n",
    "print(\"Test Accuracy:\",history[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model()\n",
    "best_model.load_weights('model/CNN/best_model.h5')\n",
    "best_model.compile(loss='binary_corssentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "predictions_1 = model.predict(preds)\n",
    "predictions_2 = best_model.predict(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_label)):\n",
    "    print('label : {}, prediction : {}'.format(pred_label[i], predictions_1[i]))"
   ]
  }
 ]
}